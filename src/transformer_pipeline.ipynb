{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felsal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/felsal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from lr.models.transformers.util import load_and_cache_examples\n",
    "from torch.utils.data import TensorDataset\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, n_gpu):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed(42, n_gpu=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "model = BertForSequenceClassification.from_pretrained(pretrained_weights, num_labels = 3)\n",
    "\n",
    "\n",
    "hyperparams = {\"local_rank\": -1,\n",
    "               \"max_seq_length\": 150,\n",
    "               \"overwrite_cache\": False,\n",
    "               \"cached_path\":\"data/toy/\",\n",
    "               \"train_path\": \"data/toy/train.csv\",\n",
    "               \"dev_path\":\"data/toy/dev.csv\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_and_cache_examples(hyperparams, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = load_and_cache_examples(hyperparams, tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# params\n",
    "\n",
    "train_dataset = train_dataset\n",
    "model = model\n",
    "tokenizer = tokenizer\n",
    "\n",
    "hyperparams = {\"local_rank\": -1,\n",
    "               \"max_seq_length\": 128,\n",
    "               \"overwrite_cache\": False,\n",
    "               \"cached_path\":\"data/toy/\",\n",
    "               \"train_path\": \"data/toy/train.csv\",\n",
    "               \"dev_path\":\"data/toy/dev.csv\",\n",
    "               \"num_train_epochs\":3.0,\n",
    "               \"per_gpu_train_batch_size\":8,\n",
    "               \"per_gpu_eval_batch_size\":8,\n",
    "               \"gradient_accumulation_steps\": 1,\n",
    "               \"learning_rate\":5e-5,\n",
    "               \"weight_decay\":0.0,\n",
    "               \"adam_epsilon\": 1e-8,\n",
    "               \"max_grad_norm\": 1.0,\n",
    "               \"max_steps\": -1,\n",
    "               \"warmup_steps\": 0,\n",
    "               \"save_steps\": 500,\n",
    "               \"no_cuda\":True,\n",
    "               \"n_gpu\":1,\n",
    "               \"model_name_or_path\":\"bert\",\n",
    "               \"output_dir\":\"bert\",\n",
    "               \"random_state\": 1234,\n",
    "               \"fp16\":False,\n",
    "               \"fp16_opt_level\":\"01\",\n",
    "               \"device\":\"cpu\",\n",
    "               \"model_type\": \"bert\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "# script\n",
    "local_rank = hyperparams[\"local_rank\"]\n",
    "per_gpu_train_batch_size = hyperparams[\"per_gpu_train_batch_size\"]\n",
    "n_gpu = hyperparams[\"n_gpu\"]\n",
    "max_steps = hyperparams[\"max_steps\"]\n",
    "num_train_epochs = hyperparams[\"num_train_epochs\"]\n",
    "gradient_accumulation_steps = hyperparams[\"gradient_accumulation_steps\"]\n",
    "weight_decay = hyperparams[\"weight_decay\"]\n",
    "learning_rate = hyperparams[\"learning_rate\"]\n",
    "adam_epsilon = hyperparams[\"adam_epsilon\"]\n",
    "warmup_steps = hyperparams[\"warmup_steps\"]\n",
    "seed = hyperparams[\"random_state\"]\n",
    "device = hyperparams[\"device\"]\n",
    "model_type = hyperparams[\"model_type\"]\n",
    "\n",
    "output_dir = hyperparams[\"output_dir\"]\n",
    "fp16_opt_level = hyperparams[\"fp16_opt_level\"] \n",
    "fp16 = hyperparams[\"fp16\"] \n",
    "\n",
    "model_name_or_path = hyperparams[\"model_name_or_path\"]\n",
    "opt_path = os.path.join(model_name_or_path, \"optimizer.pt\")\n",
    "sche_path = os.path.join(model_name_or_path, \"scheduler.pt\")\n",
    "\n",
    "\n",
    "\n",
    "train_batch_size = per_gpu_train_batch_size * max(1, n_gpu)\n",
    "\n",
    "if local_rank == -1:\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "else:\n",
    "    DistributedSampler(train_dataset)\n",
    "    \n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=train_batch_size)\n",
    "\n",
    "if max_steps > 0:\n",
    "    t_total = max_steps\n",
    "    num_train_epochs = max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
    "else:\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "    \n",
    "    \n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(\n",
    "        nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=learning_rate,\n",
    "                  eps=adam_epsilon)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=t_total)\n",
    "\n",
    "# Check if saved optimizer or scheduler states exist\n",
    "if os.path.isfile(opt_path) and os.path.isfile(sche_path):\n",
    "    # Load in optimizer and scheduler states\n",
    "    optimizer.load_state_dict(torch.load(opt_path))\n",
    "    scheduler.load_state_dict(torch.load(sche_path))\n",
    "    \n",
    "if fp16:\n",
    "    try:\n",
    "        from apex import amp\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "    model, optimizer = amp.initialize(\n",
    "        model, optimizer, opt_level=fp16_opt_level)\n",
    "    \n",
    "# multi-gpu training (should be after apex fp16 initialization)\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Distributed training (should be after apex fp16 initialization)\n",
    "if local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True)\n",
    "    \n",
    "# Train!\n",
    "logging.info(\"***** Running training *****\")\n",
    "logging.info(\"  Num examples = %d\", len(train_dataset))\n",
    "logging.info(\"  Num Epochs = %d\", num_train_epochs)\n",
    "logging.info(\"  Instantaneous batch size per GPU = %d\", per_gpu_train_batch_size)\n",
    "logging.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\", train_batch_size\n",
    "    * gradient_accumulation_steps\n",
    "    * (torch.distributed.get_world_size() if local_rank != -1 else 1))\n",
    "logging.info(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n",
    "logging.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "global_step = 0\n",
    "epochs_trained = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "\n",
    "# Check if continuing training from a checkpoint\n",
    "if os.path.exists(model_name_or_path) and model_name_or_path.find(\"checkpoints\") > 0:\n",
    "    # set global_step to gobal_step of last saved checkpoint from model\n",
    "    # path\n",
    "    global_step = int(model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "    epochs_trained = global_step // (len(train_dataloader) //\n",
    "                                     gradient_accumulation_steps)\n",
    "    steps_trained_in_current_epoch = global_step % (\n",
    "        len(train_dataloader) // gradient_accumulation_steps)\n",
    "\n",
    "    logging.info(\n",
    "        \"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "    logging.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "    logging.info(\"  Continuing training from global step %d\", global_step)\n",
    "    logging.info(\n",
    "        \"  Will skip the first %d steps in the first epoch\",\n",
    "        steps_trained_in_current_epoch)\n",
    "\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "model.zero_grad()\n",
    "set_seed(seed, n_gpu=n_gpu) # Added here for reproductibility\n",
    "train_iterator = trange(epochs_trained,\n",
    "                        int(num_train_epochs),\n",
    "                        desc=\"Epoch\",\n",
    "                        disable=local_rank not in [-1, 0])\n",
    "\n",
    "\n",
    "for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\",\n",
    "                          disable=local_rank not in [-1, 0])\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        # Skip past any already trained steps if resuming training\n",
    "        if steps_trained_in_current_epoch > 0:\n",
    "            steps_trained_in_current_epoch -= 1\n",
    "            continue\n",
    "        model.train()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0],\n",
    "                  \"attention_mask\": batch[1],\n",
    "                  \"labels\": batch[3]}\n",
    "        # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
    "        if model_type != \"distilbert\":\n",
    "            inputs[\"token_type_ids\"] = (batch[2] if model_type in [\n",
    "                        \"bert\", \"xlnet\", \"albert\"] else None)\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        break\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.177870512008667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(\"example.log\"):\n",
    "#     os.remove(\"example.log\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

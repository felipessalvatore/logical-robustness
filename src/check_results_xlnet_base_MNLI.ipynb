{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for XLNET when applying syn tranformation to both premise and hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML \n",
    "from lr.analysis.util import get_ts_from_results_folder \n",
    "from lr.analysis.util import get_rho_stats_from_result_list\n",
    "from lr.stats.h_testing import get_ks_stats_from_p_values_compared_to_uniform_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 10.22it/s]\n"
     ]
    }
   ],
   "source": [
    "all_accs = []\n",
    "all_transformed_accs = []\n",
    "all_paired_t_p_values = []\n",
    "all_dev_plus_diff = []\n",
    "all_time = []\n",
    "\n",
    "m_name = \"xlnet_base\"\n",
    "folder = \"mnli\"\n",
    "\n",
    "test_repetitions = 2\n",
    "batchs = list(range(1, test_repetitions + 1))\n",
    "# batchs = [1,2,3,4,5,6,7,8, 10, 11,12]\n",
    "\n",
    "for i in tqdm(batchs):\n",
    "    test_accuracy  = get_ts_from_results_folder(path=\"results/{}/{}/syn_p_h/batch{}/\".format(folder,m_name, i),\n",
    "                                                 stat=\"test_accuracy\")\n",
    "    \n",
    "    transformed_test_accuracy = get_ts_from_results_folder(path=\"results/{}/{}/syn_p_h/batch{}/\".format(folder,m_name, i),\n",
    "                                                           stat=\"transformed_test_accuracy\")\n",
    "    \n",
    "    paired_t_p_value  = get_ts_from_results_folder(path=\"results/{}/{}/syn_p_h/batch{}/\".format(folder,m_name, i),\n",
    "                                                    stat=\"paired_t_p_value\")\n",
    "    \n",
    "    diff  = get_ts_from_results_folder(path=\"results/{}/{}/syn_p_h/batch{}/\".format(folder,m_name, i),\n",
    "                                                    stat=\"dev_plus_accuracy_difference\")\n",
    "    \n",
    "    t_time  = get_ts_from_results_folder(path=\"results/{}/{}/syn_p_h/batch{}/\".format(folder,m_name,i),\n",
    "                                                    stat=\"test_time\")\n",
    "\n",
    "    \n",
    "    all_accs.append(test_accuracy)\n",
    "    all_transformed_accs.append(transformed_test_accuracy)\n",
    "    all_paired_t_p_values.append(paired_t_p_value)\n",
    "    all_dev_plus_diff.append(diff)\n",
    "    all_time.append(t_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2\n",
       "0.2    2\n",
       "0.4    2\n",
       "0.5    1\n",
       "0.6    2\n",
       "0.8    1\n",
       "1.0    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(all_accs,1)\n",
    "df.columns = batchs\n",
    "(df > 0.4).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7857142857142857"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df > 0.4).sum(1).sum() / (df.shape[0] * df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3142857142857143"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df > 0.4).sum(1).sum() / 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.809589</td>\n",
       "      <td>0.816817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.812694</td>\n",
       "      <td>0.812287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.813865</td>\n",
       "      <td>0.817224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.803278</td>\n",
       "      <td>0.353336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.806688</td>\n",
       "      <td>0.803431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.806790</td>\n",
       "      <td>0.328447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.353336</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1         2\n",
       "0.0  0.809589  0.816817\n",
       "0.2  0.812694  0.812287\n",
       "0.4  0.813865  0.817224\n",
       "0.5  0.803278  0.353336\n",
       "0.6  0.806688  0.803431\n",
       "0.8  0.806790  0.328447\n",
       "1.0  0.353336  0.802565"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dffsf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-26f693639325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdffsf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dffsf' is not defined"
     ]
    }
   ],
   "source": [
    "dffsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = pd.concat(all_time,1).sum().sum()\n",
    "n_params = 125238531\n",
    "print(\"Time for all experiments = {:.1f} hours\".format(total_time))\n",
    "print(\"Number of paramaters for RoBERTa = {}\".format(n_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos, mean_acc, error_acc, _ = get_rho_stats_from_result_list(all_accs)\n",
    "\n",
    "_, mean_acc_t, error_acc_t, _ = get_rho_stats_from_result_list(all_transformed_accs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.errorbar(rhos, mean_acc, yerr=error_acc, fmt='-o', label=\"original test data\");\n",
    "ax.errorbar(rhos, mean_acc_t, yerr=error_acc_t, fmt='-o', label=\"transformed test data\");\n",
    "ax.legend(loc=\"best\");\n",
    "ax.set_xlabel(r\"$\\rho$\", fontsize=14);\n",
    "ax.set_ylabel(\"accuracy\", fontsize=14);\n",
    "ax.set_title(\"ALBERT accuracy\\n\\ndataset: SNLI\\ntransformation: synonym substitution\\ntest repetitions: {}\\n\".format(test_repetitions));\n",
    "fig.tight_layout()\n",
    "# fig.savefig('figs/albert_base_acc_snli_syn_p_h.png', bbox_inches=None, pad_inches=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos, mean_p_values, error_p_values, min_p_values = get_rho_stats_from_result_list(all_paired_t_p_values)\n",
    "\n",
    "alpha = 0.05\n",
    "alpha_adj = alpha / test_repetitions\n",
    "\n",
    "rejected_ids = []\n",
    "remain_ids = []\n",
    "\n",
    "for i,p in enumerate(min_p_values):\n",
    "    if p < alpha_adj:\n",
    "        rejected_ids.append(i)\n",
    "    else:\n",
    "        remain_ids.append(i)\n",
    "        \n",
    "rhos_rejected = rhos[rejected_ids]\n",
    "rhos_remain = rhos[remain_ids]\n",
    "y_rejected = mean_p_values[rejected_ids]\n",
    "y_remain = mean_p_values[remain_ids]\n",
    "error_rejected = error_p_values[rejected_ids]\n",
    "error_remain = error_p_values[remain_ids]\n",
    "\n",
    "title_msg = \"AlBERT p-values\\n\\ndataset:\"\n",
    "title_msg += \"SNLI\\ntransformation: synonym substitution\\ntest repetitions: {}\\n\".format(test_repetitions)\n",
    "title_msg += \"significance level = {:.1%} \\n\".format(alpha)\n",
    "title_msg += \"adjusted significance level = {:.2%} \\n\".format(alpha_adj)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.errorbar(rhos_rejected, y_rejected, yerr=error_rejected, fmt='o', linewidth=0.50, label=\"at least one p-value is smaller than {:.2%}\".format(alpha_adj));\n",
    "ax.errorbar(rhos_remain, y_remain, yerr=error_remain, fmt='o', linewidth=0.50, label=\"all p-values are greater than {:.2%}\".format(alpha_adj));\n",
    "ax.legend(loc=\"best\");\n",
    "ax.set_xlabel(r\"$\\rho$\", fontsize=14);\n",
    "ax.set_ylabel(\"p-value\", fontsize=14);\n",
    "ax.set_title(title_msg);\n",
    "fig.tight_layout()\n",
    "fig.tight_layout()\n",
    "# fig.savefig('figs/albert_p_values_snli_syn_p_h.png', bbox_inches=None, pad_inches=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos, diff, _,_ = get_rho_stats_from_result_list(all_dev_plus_diff)\n",
    "_, test_acc, _,_ = get_rho_stats_from_result_list(all_accs)\n",
    "_, test_acc_t, _,_ = get_rho_stats_from_result_list(all_transformed_accs)\n",
    "test_diff = np.abs(test_acc - test_acc_t)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.errorbar(rhos, diff, fmt='-o', label=\"validation\");\n",
    "ax.errorbar(rhos, test_diff, fmt='-o', label=\"test\");\n",
    "\n",
    "ax.legend(loc=\"best\");\n",
    "ax.set_xlabel(r\"$\\rho$\", fontsize=14);\n",
    "ax.set_ylabel(\"average accuracy difference\", fontsize=14);\n",
    "ax.set_title(\"Albert accuracy difference\\n\\ndataset: SNLI\\ntransformation: synonym substitution\\ntest repetitions: {}\\n\".format(test_repetitions));\n",
    "fig.tight_layout()\n",
    "# fig.savefig('figs/albert_acc_diff_snli_syn_p_h.png', bbox_inches=None, pad_inches=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the best $\\rho$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_min = np.argmin(diff)\n",
    "min_rho = rhos[id_min]\n",
    "min_rho_test_acc = test_acc[id_min]\n",
    "min_rho_transformed_test_acc = test_acc_t[id_min]\n",
    "test_accuracy_loss_pct = np.round(((min_rho_test_acc  - test_acc[0]) / test_acc[0]) * 100, 1)\n",
    "\n",
    "analysis = {\"dataset\":\"mnli\",\n",
    "            \"model\": \"ALBERT\",\n",
    "            \"rho\":min_rho,\n",
    "            \"test_accuracy_loss_pct\": test_accuracy_loss_pct,\n",
    "            \"average_test_accuracy\": min_rho_test_acc,\n",
    "            \"average_transformed_test_accuracy\": min_rho_transformed_test_acc,\n",
    "            \"combined_accuracy\": np.mean([min_rho_test_acc,min_rho_transformed_test_acc])}\n",
    "analysis = pd.DataFrame(analysis, index=[0])\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
